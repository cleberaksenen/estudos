# 游뱄 Machine Learning - regress칚o
## Regress칚o linear

y = a + bX + 풧

1. y = vari치vel dependente (ex: pre칞o da casa)
2. X = vari치vel independente (ex: tamanho da casa)
3. a = intercepto da reta (valor de Y quando X = 0)
4. b = coeficiente angular (quanto Y varia a cada aumento de 1 unidade em X)
5. 풧 = erro (diferen칞a entre valor previsto e real)

![RegressaoLinear](../imagens/image-08.png)

### Qual a melhor reta para representar os dados? (Par칙metros: a = ?, b = ?)
Erro = verdadeiro valor - valor do modelo

![RegressaoLinear2](../imagens/image-09.png)

-> Para ter Erros positivos, eleva-se ao quadrado

- Temos a soma dos erros quadraticos:

![RegressaoLinear3](../imagens/image-10.png)

-> Quanto menor a soma, melhor o modelo

- Utilizando mais vari치veis:

![RegressaoLinear4](../imagens/image-11.png)

#### Como minimizar os erros quadr치ticos?
-> Encontrando os m칤nimos quadr치ticos!


